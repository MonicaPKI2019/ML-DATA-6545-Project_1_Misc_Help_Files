{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.4.3"},"colab":{"provenance":[{"file_id":"18JcJGDQNMnJfASFOhBjc5hcvYDSbOyue","timestamp":1677264476639}]}},"cells":[{"cell_type":"code","metadata":{"id":"-2mHsjZH1ehZ","executionInfo":{"status":"ok","timestamp":1677275380983,"user_tz":300,"elapsed":726,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":["# Recursive Feature Elimination\n","from sklearn import datasets\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","import pandas as pd # to see the dataset as a dataframe we will need pandas"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfZHidgf1eh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677275380985,"user_tz":300,"elapsed":24,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"939d48ca-2aa1-4280-a0ae-bf4e2ce6ac05"},"source":["# load the iris datasets\n","dataset = datasets.load_iris()\n","# create a base classifier used to evaluate a subset of attributes\n","model = LogisticRegression(max_iter=1000)\n","# create the RFE model and select 3 attributes\n","rfe = RFE(model, n_features_to_select= 3) # fix the 3 to the proper parameter \n","rfe = rfe.fit(dataset.data, dataset.target)\n","# summarize the selection of the attributes\n","print(rfe.support_)\n","print(rfe.ranking_)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[False  True  True  True]\n","[2 1 1 1]\n"]}]},{"cell_type":"markdown","source":["** After creating the pandas dataframe from the arrays the top features using RFE with n features set to 3 are sepal width (cm),\tpetal length (cm), & \tpetal width (cm)**"],"metadata":{"id":"7nWFQ2XxUxto"}},{"cell_type":"code","source":["print(dataset) # they are numpy arrays"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I71aMRRORFt2","executionInfo":{"status":"ok","timestamp":1677275380986,"user_tz":300,"elapsed":23,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"4346ab48-74fe-4a8a-d654-b6e0816e04e7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{'data': array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.2],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.6, 1.4, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"]}]},{"cell_type":"code","source":["# https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset\n","iris_data = pd.DataFrame(data = dataset.data, columns = dataset.feature_names) # use the attributes, like in MINIST Mini Project 2 \n","#iris_data_target = pd.DataFrame(data = dataset.data, columns = dataset.target)\n","iris_data['target'] = pd.Series(dataset.target)\n","iris_data.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Nm6Dy_uSRbk9","executionInfo":{"status":"ok","timestamp":1677275380986,"user_tz":300,"elapsed":19,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"ba3b689b-9fcd-4a14-94ad-18b864d39a59"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n","0                5.1               3.5                1.4               0.2   \n","1                4.9               3.0                1.4               0.2   \n","2                4.7               3.2                1.3               0.2   \n","3                4.6               3.1                1.5               0.2   \n","4                5.0               3.6                1.4               0.2   \n","\n","   target  \n","0       0  \n","1       0  \n","2       0  \n","3       0  \n","4       0  "],"text/html":["\n","  <div id=\"df-0e351546-8a3e-43ca-a8aa-9ba6a315fde4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal length (cm)</th>\n","      <th>sepal width (cm)</th>\n","      <th>petal length (cm)</th>\n","      <th>petal width (cm)</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e351546-8a3e-43ca-a8aa-9ba6a315fde4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0e351546-8a3e-43ca-a8aa-9ba6a315fde4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0e351546-8a3e-43ca-a8aa-9ba6a315fde4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"6BhoJvvw1eiK"},"source":["Above results showing that the latter 3 features are selected in the model (`True` means this feature is selected). The second row shows the ranking of the features if you want to use the __filter__ method.\n","\n","Note that you do not need to check the performance (f1-score) or goodness-of-fit (AUC) in your model here since `RFE` automatically select the best subset of features.\n","\n","However, we abitrarily select __3 features__ from the original 4. This is not scientific. What you should do is that test different number of features in the subset. You can use the code below."]},{"cell_type":"code","metadata":{"id":"7sT-_du71eiO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677275380987,"user_tz":300,"elapsed":18,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"0c77a044-8f29-41d7-806c-1dcb24fd3426"},"source":["# Checking how many features this dataset have\n","print(len(dataset.data[0]))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"code","metadata":{"id":"B7hMWN8x1eib","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677275381437,"user_tz":300,"elapsed":462,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"1513014c-73aa-4025-bc23-9e6322049560"},"source":["for i in range(1, len(dataset.data[0])+1):\n","    print(i)\n","    # create a base classifier used to evaluate a subset of attributes\n","    model = LogisticRegression()\n","    # create the RFE model and select 3 attributes\n","    rfe = RFE(model, n_features_to_select= i)\n","    rfe = rfe.fit(dataset.data, dataset.target)\n","    # summarize the selection of the attributes\n","    print('Model with the best', i, 'features')\n","    print(rfe.support_)\n","    print(rfe.ranking_)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Model with the best 1 features\n","[False False  True False]\n","[4 3 1 2]\n","2\n","Model with the best 2 features\n","[False False  True  True]\n","[3 2 1 1]\n","3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Model with the best 3 features\n","[False  True  True  True]\n","[2 1 1 1]\n","4\n","Model with the best 4 features\n","[ True  True  True  True]\n","[1 1 1 1]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"markdown","source":["**The top feature of RFE set to 1 feature to select is petal length (cm)**\n","\n","**The top two features of RFE set to 2 features to select are petal length (cm)\t& petal width (cm)**\n","\n","**The top three features of RFE set to 3 features to select are sepal width (cm),\tpetal length (cm), & petal width (cm)** \n","\n","**The top features of RFE set to 4 features to select are all the features sepal length (cm),\tsepal width (cm),\tpetal length (cm), &\tpetal width (cm)**\n"],"metadata":{"id":"KpH6vnp1YDi_"}},{"cell_type":"markdown","metadata":{"id":"aNZlUYkK1ein"},"source":["Then you should have different subsets of selected into the __Evaluation code__ provided to check model performance.\n","\n","`RFE` is like a black box - you can use a different method which you can look into the feature selection process, and have more control.\n","\n","Decision tree based classifiers, such as random forest classifier or extra trees classifier, can report feature importance - which you can use as a threshold to filter features. \n","\n","See code below - in code below, we use extra trees classifier to report feature importance, and use an abitrary `0.4` value to filter the features. The idea is whatever features passing the threshold would be selected."]},{"cell_type":"code","metadata":{"id":"G2ZM6Khn1eio","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677275381438,"user_tz":300,"elapsed":21,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}},"outputId":"d7bd24f2-35ec-4172-f9e4-1138d55cda59"},"source":["# Feature Importance\n","from sklearn import datasets\n","from sklearn import metrics\n","from sklearn.ensemble import ExtraTreesClassifier\n","# load the iris datasets\n","dataset = datasets.load_iris()\n","# fit an Extra Trees model to the data\n","model = ExtraTreesClassifier()\n","model.fit(dataset.data, dataset.target)\n","# display the relative importance of each attribute\n","print(model.feature_importances_ > .4)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[False False False  True]\n"]}]},{"cell_type":"markdown","metadata":{"id":"dVUFXn_p1eiy"},"source":["Above results show that the 3rd and 4th features passed our threshold, hence should be selected.\n","\n","Of course, abitrarily select the threshold at `.4` is not scientific. You should also try different values in a for loop.\n","\n","# References\n","- [Kaggle - Step Forward Feature Selection: A Practical Example in Python](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)\n","- [MLM - Feature Selection in Python with Scikit-Learn](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/)"]},{"cell_type":"markdown","source":["**The top features using decision trees set to a importance score >.4 are petal length (cm)\t& petal width (cm)**"],"metadata":{"id":"KQdZ-EsYZYsb"}},{"cell_type":"code","metadata":{"id":"SvbaP2k51ei1","executionInfo":{"status":"ok","timestamp":1677275381440,"user_tz":300,"elapsed":14,"user":{"displayName":"Monica Willson","userId":"18384632992554910821"}}},"source":[],"execution_count":7,"outputs":[]}]}